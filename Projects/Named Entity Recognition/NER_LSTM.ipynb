{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(33) # every time you use the model will get the same accuracy\n"
      ],
      "metadata": {
        "id": "s9Zih1YPTAuO"
      },
      "id": "s9Zih1YPTAuO",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to lead the data\n",
        "def load_data(file_path):\n",
        "    with open(file_path,'r') as file:\n",
        "        data = np.array([line.strip() for line in file.readlines()])\n",
        "    return data"
      ],
      "metadata": {
        "id": "xlknd4cNULYE"
      },
      "id": "xlknd4cNULYE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import our data\n",
        "train_sentences = load_data('/content/drive/MyDrive/train/sentences.txt')\n",
        "train_labels = load_data('/content/drive/MyDrive/train/labels.txt')\n",
        "val_sentences = load_data('/content/drive/MyDrive/val/sentences.txt')\n",
        "val_labels = load_data('/content/drive/MyDrive/val/labels.txt')\n",
        "test_sentences = load_data('/content/drive/MyDrive/test/sentences.txt')\n",
        "test_labels = load_data('/content/drive/MyDrive/test/labels.txt')\n"
      ],
      "metadata": {
        "id": "H2rV0heaURZD"
      },
      "id": "H2rV0heaURZD",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vectorizer(sentences):\n",
        "    tf.keras.utils.set_random_seed(33) # we know it before\n",
        "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize=None, max_tokens=None, output_mode='int')\n",
        "    # first dont make standardize for all senenteces second dont limit words got third all in integer output\n",
        "    sentence_vectorizer.adapt(sentences)  # like transform\n",
        "    vocab = sentence_vectorizer.get_vocabulary()  # get all vocb after vectrorize sentences\n",
        "    return sentence_vectorizer, vocab\n",
        "\n",
        "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences) # like fit"
      ],
      "metadata": {
        "id": "8npv3B-MUtMS"
      },
      "id": "8npv3B-MUtMS",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store all unique tags from labels by alphabatic sort\n",
        "def get_tags(labels):\n",
        "    tag_set = set()\n",
        "    for el in labels:\n",
        "        for tag in el.split(\" \"):\n",
        "            tag_set.add(tag)\n",
        "    tag_list = list(tag_set)\n",
        "    tag_list.sort()\n",
        "    return tag_list\n",
        "\n",
        "tags = get_tags(train_labels)\n"
      ],
      "metadata": {
        "id": "mh9tQjmJZV-c"
      },
      "id": "mh9tQjmJZV-c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map each tag\n",
        "def make_tag_map(tags):\n",
        "    return {tag: i for i, tag in enumerate(tags)}\n",
        "\n",
        "tag_map = make_tag_map(tags)"
      ],
      "metadata": {
        "id": "lcgQPHNdapaw"
      },
      "id": "lcgQPHNdapaw",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will here make mapping for all labels using the mapped tags and also we will make padding (post meaing in the end ) to ensure that all have same dimension\n",
        "def label_vectorizer(labels, tag_map):\n",
        "    label_ids = []\n",
        "    for element in labels:\n",
        "        tokens = element.split(\" \")\n",
        "        element_ids = [tag_map.get(token, -1) for token in tokens]\n",
        "        label_ids.append(element_ids)\n",
        "    return np.array(tf.keras.preprocessing.sequence.pad_sequences(label_ids, padding='post', value=-1))\n"
      ],
      "metadata": {
        "id": "_wxD66AGawqX"
      },
      "id": "_wxD66AGawqX",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates a TensorFlow dataset from sentences and their corresponding labels by vectorizing them into numerical representations.\n",
        "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
        "    sentences_ids = sentence_vectorizer(sentences)\n",
        "    labels_ids = label_vectorizer(labels, tag_map = tag_map)\n",
        "    return tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
        "\n",
        "train_dataset = generate_dataset(train_sentences, train_labels, sentence_vectorizer, tag_map)\n",
        "val_dataset = generate_dataset(val_sentences, val_labels, sentence_vectorizer, tag_map)\n",
        "test_dataset = generate_dataset(test_sentences, test_labels, sentence_vectorizer, tag_map)\n",
        "\n",
        "g_vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "1slRYFGmfhZD"
      },
      "id": "1slRYFGmfhZD",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define our model\n",
        "def NER(len_tags, vocab_size, embedding_dim=50):\n",
        "    model = tf.keras.Sequential(name='sequential')\n",
        "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, mask_zero=True))\n",
        "    model.add(tf.keras.layers.LSTM(units=embedding_dim, return_sequences=True))\n",
        "    model.add(tf.keras.layers.Dense(units=len_tags, activation=tf.nn.log_softmax))\n",
        "    return model"
      ],
      "metadata": {
        "id": "AovGdL4-h7gL"
      },
      "id": "AovGdL4-h7gL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function computes the masked loss by applying Sparse Categorical Crossentropy while ignoring the -1 values in y_true.\n",
        "# It calculates the average loss only for valid entries to ensure accurate model training.\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    # Ignore the -1 values in y_true before calculating the loss\n",
        "    # This will prevent the error\n",
        "    valid_indices = tf.where(tf.not_equal(y_true, -1))\n",
        "    y_true_valid = tf.gather_nd(y_true, valid_indices)\n",
        "    y_pred_valid = tf.gather_nd(y_pred, valid_indices)\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true_valid, y_pred_valid)  # Calculate loss only on valid indices\n",
        "    loss = tf.cast(loss, dtype=tf.float32)\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "m9E4dnxwiA6j"
      },
      "id": "m9E4dnxwiA6j",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates the masked accuracy by comparing the true labels (y_true) with the predicted classes (y_pred) while ignoring the -1 values.\n",
        "# It computes the ratio of correctly predicted labels to the total valid labels, ensuring accurate evaluation of the model's performance.\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    mask = tf.cast(tf.not_equal(y_true, -1), dtype=tf.float32)\n",
        "    y_pred_class = tf.math.argmax(y_pred, axis=-1)\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    y_pred_class = tf.cast(y_pred_class, tf.int32)\n",
        "    matches_true_pred = tf.equal(y_true, y_pred_class)\n",
        "    matches_true_pred = tf.cast(matches_true_pred, tf.float32)\n",
        "    matches_true_pred *= mask\n",
        "    return tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "FbO9F4OQjb88"
      },
      "id": "FbO9F4OQjb88",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an NER model with the number of tags and vocabulary size.\n",
        "model = NER(len(tag_map), len(vocab))\n",
        "# Compile the model with the Adam optimizer, masked loss function, and masked accuracy metric.\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=masked_loss, metrics=[masked_accuracy])\n",
        "tf.keras.utils.set_random_seed(33)  # we explain it before\n",
        "BATCH_SIZE = 64 # for training\n",
        "\n",
        "model.fit(train_dataset.batch(BATCH_SIZE), validation_data=val_dataset.batch(BATCH_SIZE), shuffle=True, epochs=2) # fitting the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPdW12NRju05",
        "outputId": "e2d8dc3c-15ac-4624-9323-82f922a700b8"
      },
      "id": "yPdW12NRju05",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 109ms/step - loss: 0.4594 - masked_accuracy: 0.8952 - val_loss: 0.1393 - val_masked_accuracy: 0.9573\n",
            "Epoch 2/2\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 107ms/step - loss: 0.1299 - masked_accuracy: 0.9612 - val_loss: 0.1359 - val_masked_accuracy: 0.9584\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7946b015d090>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare test data\n",
        "test_sentences_id = sentence_vectorizer(test_sentences)\n",
        "test_labels_id = label_vectorizer(test_labels, tag_map)\n",
        "y_true = test_labels_id\n",
        "y_pred = model.predict(test_sentences_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asZRTmZRktu6",
        "outputId": "872ac472-4426-490c-f7a2-93e32ff0790f"
      },
      "id": "asZRTmZRktu6",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The model's accuracy in test set is: {masked_accuracy(y_true,y_pred).numpy():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuLm0PJi0fU7",
        "outputId": "80637ddb-9529-4952-f0cc-b431e9f99fdb"
      },
      "id": "QuLm0PJi0fU7",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model's accuracy in test set is: 0.9576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict with your own sentence\n",
        "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
        "    sentence_vectorized = sentence_vectorizer(sentence)\n",
        "    sentence_vectorized = tf.expand_dims(sentence_vectorized, 0)\n",
        "    output = model(sentence_vectorized)\n",
        "    outputs = np.argmax(output, axis=-1)\n",
        "    outputs = outputs[0]\n",
        "    labels = list(tag_map.keys())\n",
        "    pred = [labels[tag_idx] for tag_idx in outputs]\n",
        "    return pred\n",
        "\n"
      ],
      "metadata": {
        "id": "UjyM9onISd9p"
      },
      "id": "UjyM9onISd9p",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"ahmed want to visit pyramids in egypt next Year\"\n",
        "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
        "for x,y in zip(sentence.split(' '), predictions):\n",
        "    if y != 'O':\n",
        "        print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLJMC9oW0wjq",
        "outputId": "6a7e543a-27d0-4373-94db-c3bd47c9fd48"
      },
      "id": "NLJMC9oW0wjq",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahmed B-per\n",
            "egypt B-geo\n",
            "Year B-tim\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}